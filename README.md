# ReferGPT: Towards Zero-Shot Referring Multi-Object Tracking

[![arXiv](https://img.shields.io/badge/arXiv-2504.09195-b31b1b.svg)](https://arxiv.org/abs/2504.09195)
[![GitHub](https://img.shields.io/badge/GitHub-ReferGPT-blue)](https://github.com/Tzoulio/ReferGPT)
[![License](https://img.shields.io/badge/License-Apache%202.0-green.svg)](https://opensource.org/licenses/Apache-2.0)

**Official implementation of "ReferGPT: Towards Zero-Shot Referring Multi-Object Tracking" (CVPRW 2025)**

ReferGPT introduces a novel approach to language-guided 3D multi-object tracking by integrating CLIP vision-language understanding, Large Language Models (LLMs), and Vision-Language Models (VLMs) for zero-shot referring multi-object tracking. Our method enables tracking objects based on natural language descriptions without requiring task-specific training.

<p align="center">
<img src="img/in_front_cars.gif"/>
<img src="img/same_direction.gif"/>
<img src="img/black_cars.gif"/>
</p>

## Methodology 
<div align="center">
  <img src="./img/main_architecture.png">
</div>

## üöÄ Key Features

- **Zero-Shot Language-Guided Tracking**: Track objects using natural language descriptions without task-specific training
- **Multimodal Architecture**: Combines CLIP, LLM, and VLM for comprehensive understanding
- **3D Multi-Object Tracking**: Full 3D tracking with Kalman filtering and advanced association
- **Multiple Detector Support**: Compatible with PV-RCNN, Point-RCNN, SECOND-IOU, CasA, VirConv, and more
- **HOTA Evaluation**: Comprehensive evaluation using state-of-the-art HOTA metrics

## üìã Table of Contents

- [Installation](#installation)
- [Quick Start](#quick-start)
- [Configuration](#configuration)
- [Results](#results)
- [Citation](#citation)

## üõ†Ô∏è Installation

For detailed installation instructions, including environment setup, data preparation, and troubleshooting, please see our comprehensive [Installation Guide](docs/installation.md).

**Quick Setup:**
```bash
git clone https://github.com/Tzoulio/ReferGPT.git
cd ReferGPT
conda create -n refergpt python=3.12
conda activate refergpt
pip install -r requirements.txt
```

**Requirements:**
- Python 3.12+ (tested with 3.12.9)
- CUDA 11.0+ or CUDA 12.0+
- Linux (Ubuntu 18.04+)

For complete setup instructions and data downloads, visit: **[üìñ Installation Guide](docs/installation.md)**

## üöÄ Quick Start

### Basic Usage

1. **Configure paths in config file**
```bash
# Edit the configuration file
nano config/global/cfg_refergpt.yaml
```

Update the following paths:
- `dataset_path`: Path to your KITTI dataset
- `detections_path`: Path to your detection files
- `expression_path`: Path to referring expressions
- `llm_output_data_file`: Path to LLM outputs (if using pre-computed, provided in the Installation guide)

Be aware that the provided LLM outputs are specifically tailored for detections generated by CasA and the parameter settings defined in the configuration file. If new detections are introduced or parameters are changed, the framework will first attempt to match the new detections with existing llm generated captions that we provide (dataset/data/updated_casa_llm_output_data), based on bounding box coordinates (see `model/matching/trajectory_matching.py`, line 59 and onwards). If no match is found, a new llm caption will be created.  

To enable LLM output generation, update the configuration file by setting `use_llm: True` and `save_llm_output: True`.

2. **Run tracking**
```bash
# Run tracking for both cars and pedestrians
bash run_tracker.sh
```

### Advanced Usage

**Run specific tracking type:**
```bash
# For cars only
python3 main.py --cfg_file config/global/cfg_refergpt.yaml

# For pedestrians only  
python3 main.py --cfg_file config/global/cfg_refergpt_pedestrian.yaml
```

**Generate new LLM outputs:**
```bash
# Enable LLM generation in config
# Set use_llm: True and save_llm_output: True in config file
python3 main.py --cfg_file config/global/cfg_refergpt.yaml
```


### Supported VLM Models

- **Phi-3-Vision**: Lightweight and efficient (default)
- **LLaVA**: High-quality vision-language understanding
- **GPT-4V**: Premium performance (requires API key)

## üìà Results

### Refer-KITTI Benchmark Results

| Dataset | HOTA | DetA | DetRe | DetPR | AssA | AssRe | AssPr | LocA |
|---------|------|------|-------|-------|------|-------|-------|------|
| **Refer-KITTI** | 49.46 | 39.43 | 50.21 | 58.91 | 62.57 | 73.74 | 72.78 | 81.85 |
| **Refer-KITTIv2** | 30.12 | 15.69 | 21.55 | 34.41 | 59.02 | 74.59 | 68.20 | 79.76 |
| **Refer-KITTI+** | 43.44 | 29.89 | 36.59 | 56.98 | 63.60 | 75.20 | 73.27 | 82.23 |

### Performance Characteristics

- **Zero-shot Capability**: No task-specific training required
- **Language Flexibility**: Supports complex natural language queries
- **3D Accuracy**: State-of-the-art 3D tracking performance

## üôè Acknowledgments

This work builds upon the foundation of [PC3T](https://github.com/hailanyi/3D-Multi-Object-Tracker). Many thanks for their wonderful works.

## üìù Citation

If you find ReferGPT useful in your research, please consider citing:

```bibtex
@misc{chamiti2025refergptzeroshotreferringmultiobject,
      title={ReferGPT: Towards Zero-Shot Referring Multi-Object Tracking}, 
      author={Tzoulio Chamiti and Leandro Di Bella and Adrian Munteanu and Nikos Deligiannis},
      year={2025},
      eprint={2504.09195},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2504.09195}, 
}
```


## üìÑ License

This project is licensed under the Apache License 2.0 - see the [LICENSE](LICENSE) file for details.
